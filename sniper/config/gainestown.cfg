# Configuration file for Xeon X5550 Gainestown
# See http://en.wikipedia.org/wiki/Gainestown_(microprocessor)#Gainestown
# and http://ark.intel.com/products/37106

# [LINGXI]: models HMC baseline

#include nehalem

[perf_model/core]
frequency = 1.0
[perf_model/l1_icache]
passthrough = true
perfect = false
cache_size = 32
associativity = 4
address_hash = mask
replacement_policy = lru
data_access_time = 4
tags_access_time = 1
perf_model_type = parallel
writethrough = 0
shared_cores = 1

[perf_model/cache]
levels = 2

[perf_model/l1_dcache]
passthrough = true
perfect = false
cache_size = 32
associativity = 8
address_hash = mask
replacement_policy = lru
data_access_time = 4
tags_access_time = 1
perf_model_type = parallel
writethrough = 0
shared_cores = 1

[perf_model/l2_cache]
perfect = false
cache_size = 32
associativity = 8
address_hash = mask
replacement_policy = lru
data_access_time = 4
tags_access_time = 1
writeback_time = 0 # not sure about this
perf_model_type = parallel
writethrough = 0
shared_cores = 1

######## [LINGXI]: HMC-related ########
[network]
memory_model_1 = emesh_hop_by_hop

[network/emesh_hop_by_hop] 
size = "16:8"             # ":"-searated list of size for each dimension, default = auto WIDTH:HEIGHT

[network/emesh_hop_by_hop/HMC_topology]
model_hmc = true
type = grid # daisy_chain (span cubes in one dimension), or grid (two dimension connectivity)
quadrant_size = 8 # 4 or 8 valuts per quadrant
external_link_bandwidth = 120 # GB/s will be shared by multiple vaults

[perf_model/dram]
# -1 means that we have a number of distributed DRAM controllers (4 in this case)
num_controllers = -1
controllers_interleaving = 1
# DRAM access latency in nanoseconds. Should not include L1-LLC tag access time, directory access time (14 cycles = 5.2 ns),
# or network time [(cache line size + 2*{overhead=40}) / network bandwidth = 18 ns]
# Membench says 175 cycles @ 2.66 GHz = 66 ns total
latency = 33.6
per_controller_bandwidth = 10              # GB/s. HMC1.0 8, HMC2.0 10
chips_per_dimm = 8
dimms_per_controller = 4
######## [LINGXI]: HMC-related ########

[perf_model/dram_directory]
# total_entries = number of entries per directory controller.
total_entries = 1048576
associativity = 16
directory_type = full_map

[network/bus]
bandwidth = 25.6 # in GB/s. Actually, it's 12.8 GB/s per direction and per connected chip pair
ignore_local_traffic = true # Memory controllers are on-chip, so traffic from core0 to dram0 does not use the QPI links

